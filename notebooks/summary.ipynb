{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Summary*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook is a summary of the process building this ranking system. The actual coding is consists of three parts:\n",
    "1. Analysis (To get an idea of suitable ETL and ranking way)\n",
    "2. ETL scripts (The downloading and preprocessing steps)\n",
    "3. Ranking (Perform the actual ranking)\n",
    "\n",
    "### Analysis\n",
    "The notebooks of analysis part are placed in the notebooks/analysis and the related notebooks are:\n",
    "1. api.ipynb (To get an idea of how we combined the given and the external dataset)\n",
    "2. external.ipynb (To get understanding of external dataset, such as its distribution)\n",
    "3. external2.ipynb (An older version of external.ipynb)\n",
    "4. fraud.ipynb (Analysis of the special feature fraud. By using this, we proposed a way to handle this feature)\n",
    "5. outier.ipynb (Analysis of the outliers in dataset, and try possible way to remove the data we dont want)\n",
    "6. segment.ipynb (To decide segments that worth to perform ranking)\n",
    "\n",
    "### ETL\n",
    "1. read_data.ipynb (The notebook and junior version of read_data.py)\n",
    "2. download.py (First part of ETL, downloading the needed external dataset)\n",
    "3. unzip.py (Second part of ETL, unzip the dataset downloading)\n",
    "4. read_data.py (Final part of ETL, performing the actual dataset for ranking)\n",
    "\n",
    "### Ranking\n",
    "1. rank_model_explain.md (Explain the principle of our ranking model)\n",
    "2. rank_algorithm.ipynb (The notebook version of ranking with no action on feature fraud)\n",
    "3. rank_algorithm_no_fraud.ipynb (The notebook version of ranking with removing the fraud transactions for each merchant)\n",
    "4. ranking.py (Combine both with and without fraud ranking into one python program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "A few assumptions were made for this ranking system\n",
    "- There is no online purchasing so each transaction record can be linked to an actual customer\n",
    "- The merchant business area is represent by combination of its top 5 area that made the most orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess & analysis\n",
    "The data preprocessing and analysis can be subdivided into the following areas\n",
    " 1. Download and extract data\n",
    " 2. Generalized ETL\n",
    " 3. Joining datasets\n",
    " 4. External datasets\n",
    " 5. Visualizationa and outliers\n",
    "\n",
    "### NO.1 Download and extract data & process data \n",
    " - First, we read the given datasets, (i.e `transactions` , `consumers` , `merchants`) , we then checked on the data size and data types accordingly, \n",
    " we joined datasets using left outer join, repeated columns had been dropped accordingly\n",
    " - We resolve `tag` by saving merchant tags to different columns`field` , `renvenue_leve` and `take_rate`, since `take_rate` is float type\n",
    " transform all strings in \"field\" and \"revenue_level\" to lowercase\n",
    " - Then we store the curated dataframes into both `csv` and `parquet` form (`full_data`)\n",
    "\n",
    "### NO.2 External datasets\n",
    " - We select SA2(Statistical Area Level 2) data, both `income` and `population` data and find a way to link the SA2 and postcode\n",
    " - After joining these external data, removing null values results in significant loss of data nearly `20%`\n",
    " - After plotting the distribution, we decided to use median to fill the null values(missing data), mitigating this issue\n",
    "\n",
    "### NO.3 Outlier \n",
    "- We check and remove null values \n",
    "- Remove unhelpful columns (i.e `address`, `gender`)\n",
    "- We check for wrong data \n",
    " - i.e `user_id` , `consumer_id` , `postcode` need to be greater than `0`\n",
    "- Check for features, (i.e `dollar_value` must greater than 0 , `postcode` must be four digits length)\n",
    "\n",
    "\n",
    "### NO.4 Generalized ETL script\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pyspark initialization and Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "import builtins\n",
    "from cmath import nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spark\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 ass2 BNPL group 28\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants = spark.read.parquet('../data/tables/tbl_merchants.parquet')\n",
    "consumers = spark.read.parquet('../data/tables/consumer_user_details.parquet')\n",
    "transactions = spark.read.parquet('../data/tables/transactions_20210228_20210827_snapshot')\n",
    "consumers_csv = spark.read.options(header='True', inferSchema='True', delimiter='|').csv('../data/tables/tbl_consumer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>merchant_abn</th><th>dollar_value</th><th>order_id</th><th>order_datetime</th><th>consumer_id</th><th>name</th><th>tags</th><th>name</th><th>address</th><th>state</th><th>postcode</th><th>gender</th></tr>\n",
       "<tr><td>5630</td><td>60956456424</td><td>145.26081329000152</td><td>1e14adeb-8e13-44f...</td><td>2021-08-21</td><td>28242</td><td>Ultricies Digniss...</td><td>([gift, card, Nov...</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td></tr>\n",
       "<tr><td>5630</td><td>48534649627</td><td>120.25889985200416</td><td>08476339-f383-4ab...</td><td>2021-08-15</td><td>28242</td><td>Dignissim Maecena...</td><td>[[opticians, oPti...</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td></tr>\n",
       "<tr><td>5630</td><td>60956456424</td><td>135.5412540082104</td><td>aacfd47a-438b-47f...</td><td>2021-08-15</td><td>28242</td><td>Ultricies Digniss...</td><td>([gift, card, Nov...</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td></tr>\n",
       "<tr><td>5630</td><td>89932674734</td><td>95.37693966478514</td><td>6d5790c9-0eef-453...</td><td>2021-08-16</td><td>28242</td><td>Nulla Vulputate C...</td><td>((aRtist supply a...</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td></tr>\n",
       "<tr><td>5630</td><td>14089706307</td><td>440.12097711482835</td><td>43d1361a-1101-41a...</td><td>2021-08-16</td><td>28242</td><td>Donec Institute</td><td>[(computer progra...</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------------+------------------+--------------------+--------------+-----------+--------------------+--------------------+---------------+--------------------+-----+--------+-----------+\n",
       "|user_id|merchant_abn|      dollar_value|            order_id|order_datetime|consumer_id|                name|                tags|           name|             address|state|postcode|     gender|\n",
       "+-------+------------+------------------+--------------------+--------------+-----------+--------------------+--------------------+---------------+--------------------+-----+--------+-----------+\n",
       "|   5630| 60956456424|145.26081329000152|1e14adeb-8e13-44f...|    2021-08-21|      28242|Ultricies Digniss...|([gift, card, Nov...|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|\n",
       "|   5630| 48534649627|120.25889985200416|08476339-f383-4ab...|    2021-08-15|      28242|Dignissim Maecena...|[[opticians, oPti...|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|\n",
       "|   5630| 60956456424| 135.5412540082104|aacfd47a-438b-47f...|    2021-08-15|      28242|Ultricies Digniss...|([gift, card, Nov...|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|\n",
       "|   5630| 89932674734| 95.37693966478514|6d5790c9-0eef-453...|    2021-08-16|      28242|Nulla Vulputate C...|((aRtist supply a...|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|\n",
       "|   5630| 14089706307|440.12097711482835|43d1361a-1101-41a...|    2021-08-16|      28242|     Donec Institute|[(computer progra...|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|\n",
       "+-------+------------+------------------+--------------------+--------------+-----------+--------------------+--------------------+---------------+--------------------+-----+--------+-----------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## left outer join transaction data with consumers data by user_id.\n",
    "new_transaction = transactions.join(consumers, transactions.user_id == consumers.user_id, \"leftouter\").drop(consumers.user_id)\n",
    "new_transaction = new_transaction.join(merchants, new_transaction.merchant_abn == merchants.merchant_abn, \"leftouter\").drop(merchants.merchant_abn)\n",
    "new_transaction = new_transaction.join(consumers_csv, new_transaction.consumer_id == consumers_csv.consumer_id, \"leftouter\").drop(consumers_csv.consumer_id)\n",
    "new_transaction.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save tags into columns `field`, `revenue_level`, `take_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save merchant tags to different columns \"field\", \"renvenue_level\" and \"take_rate\", while \"take_rate\" is float type\n",
    "## transform all strings in \"field\" and \"revenue_level\" to lowercase\n",
    "new_transaction = new_transaction.withColumn('tags', expr(\"substring(tags, 3, length(tags)-4)\")) \\\n",
    "    .withColumn('field', split(col(\"tags\"), \"\\], \\[|\\), \\(\").getItem(0)) \\\n",
    "        .withColumn('revenue_level', split(col(\"tags\"), \"\\], \\[|\\), \\(\").getItem(1)) \\\n",
    "            .withColumn('take_rate', split(col(\"tags\"), \"\\], \\[|\\), \\(\").getItem(2)) \\\n",
    "                .withColumn('take_rate', regexp_extract(col(\"take_rate\"), r'(\\d+).(\\d+)', 0)) \\\n",
    "                    .withColumn(\"take_rate\", col('take_rate').cast(FloatType())) \\\n",
    "                        .withColumn('field', lower(col('field'))) \\\n",
    "                            .withColumn('revenue_level', lower(col('revenue_level'))) \\\n",
    "                                .drop(\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>merchant_abn</th><th>dollar_value</th><th>order_id</th><th>order_datetime</th><th>consumer_id</th><th>name</th><th>name</th><th>address</th><th>state</th><th>postcode</th><th>gender</th><th>field</th><th>revenue_level</th><th>take_rate</th></tr>\n",
       "<tr><td>5630</td><td>60956456424</td><td>145.26081329000152</td><td>1e14adeb-8e13-44f...</td><td>2021-08-21</td><td>28242</td><td>Ultricies Digniss...</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td><td>gift, card, novel...</td><td>b</td><td>4.69</td></tr>\n",
       "<tr><td>5630</td><td>48534649627</td><td>120.25889985200416</td><td>08476339-f383-4ab...</td><td>2021-08-15</td><td>28242</td><td>Dignissim Maecena...</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td><td>opticians, optica...</td><td>a</td><td>6.64</td></tr>\n",
       "<tr><td>5630</td><td>60956456424</td><td>135.5412540082104</td><td>aacfd47a-438b-47f...</td><td>2021-08-15</td><td>28242</td><td>Ultricies Digniss...</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td><td>gift, card, novel...</td><td>b</td><td>4.69</td></tr>\n",
       "<tr><td>5630</td><td>89932674734</td><td>95.37693966478514</td><td>6d5790c9-0eef-453...</td><td>2021-08-16</td><td>28242</td><td>Nulla Vulputate C...</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td><td>artist supply and...</td><td>c</td><td>1.67</td></tr>\n",
       "<tr><td>5630</td><td>14089706307</td><td>440.12097711482835</td><td>43d1361a-1101-41a...</td><td>2021-08-16</td><td>28242</td><td>Donec Institute</td><td>Philip Crawford</td><td>7487 Serrano Gard...</td><td>NT</td><td>841</td><td>Undisclosed</td><td>computer programm...</td><td>b</td><td>3.33</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------------+------------------+--------------------+--------------+-----------+--------------------+---------------+--------------------+-----+--------+-----------+--------------------+-------------+---------+\n",
       "|user_id|merchant_abn|      dollar_value|            order_id|order_datetime|consumer_id|                name|           name|             address|state|postcode|     gender|               field|revenue_level|take_rate|\n",
       "+-------+------------+------------------+--------------------+--------------+-----------+--------------------+---------------+--------------------+-----+--------+-----------+--------------------+-------------+---------+\n",
       "|   5630| 60956456424|145.26081329000152|1e14adeb-8e13-44f...|    2021-08-21|      28242|Ultricies Digniss...|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|gift, card, novel...|            b|     4.69|\n",
       "|   5630| 48534649627|120.25889985200416|08476339-f383-4ab...|    2021-08-15|      28242|Dignissim Maecena...|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|opticians, optica...|            a|     6.64|\n",
       "|   5630| 60956456424| 135.5412540082104|aacfd47a-438b-47f...|    2021-08-15|      28242|Ultricies Digniss...|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|gift, card, novel...|            b|     4.69|\n",
       "|   5630| 89932674734| 95.37693966478514|6d5790c9-0eef-453...|    2021-08-16|      28242|Nulla Vulputate C...|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|artist supply and...|            c|     1.67|\n",
       "|   5630| 14089706307|440.12097711482835|43d1361a-1101-41a...|    2021-08-16|      28242|     Donec Institute|Philip Crawford|7487 Serrano Gard...|   NT|     841|Undisclosed|computer programm...|            b|     3.33|\n",
       "+-------+------------+------------------+--------------------+--------------+-----------+--------------------+---------------+--------------------+-----+--------+-----------+--------------------+-------------+---------+"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier analysis\n",
    "- drop columns which are not helpful\n",
    "- Remove null values\n",
    "- Check for features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop unhelpful columns\n",
    "cols = ['address','gender', 'consumer_id', 'user_name', 'state']\n",
    "new_transaction = new_transaction.drop(*cols)\n",
    "\n",
    "## drop rows that have null values\n",
    "new_transaction = new_transaction.dropna()\n",
    "\n",
    "## drop transaction that has dollor value less or equal to 0\n",
    "new_transaction = new_transaction.filter((col('dollar_value') >= 0))\n",
    "\n",
    "## check order datetime to in the right range\n",
    "new_transaction = new_transaction.filter((col('order_datetime') >= '2021-02-28') & (col('order_datetime') <= '2022-08-28'))\n",
    "\n",
    "## check the consistency of postcode\n",
    "new_transaction = new_transaction.filter(length(col('postcode')) == 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining external dataset (census data)\n",
    "- We chosed Sa2 population and income as external data, joined external data to main dataset(details in `external.ipynb` & `external2.ipynb`)\n",
    "- Filter outliers after joining external data\n",
    "    - fill null values with mean/median, avoid losing too much data\n",
    "- Distribution plots(null values filled with mean/median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud\n",
    "- Fraud detection \n",
    " - Join fraud data (`consumer_fraud_probability`, `merchant_fraud_probability`) to our main dataset\n",
    " - Consider `30%` and above are fraud\n",
    "    - Add new feature \"is_fraud\" to classify whether a record is a fraud record\n",
    "    - Used NaiveBayes model (can be more detailed)\n",
    "        - high accuracy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking system \n",
    "### Aim\n",
    " - The goal of the model is to recommend to BNPL company the top N cooperative merchants that are in the long-term interest according to some specific characteristics\n",
    "\n",
    "### Merchant features we are considering \n",
    "- `transaction_count`: number of transcations made in a specified period.\n",
    "- `take_rate`: the fee charged by the BNPL firm to a merchant on a transaction. That is, for each transaction made, a certain percentage is taken by the BNPL firm.\n",
    "- `revenue_level`: `(a, b, c, d, e)` represents the level of revenue bands (unknown to groups) 'a' denotes the smallest band whilst 'e' denotes the highest revenue band.\n",
    "- `total_revenue`: the total revenue made by a merchant in a specified period\n",
    "- `mean_consumer_income`: the mean weekly income of each merchant's consumers (used to represents the puchasing power of merchants' target audience)\n",
    "- `fraud_count`: the number of transactions that are recongnized as fraud\n",
    "- `main_business_area_popu`: a sum of the number of consumers in the top five postcode areas corresponding to each merchant that has most users within these areas\n",
    "\n",
    "### Ranking model\n",
    "1. Model Theory (Implementation of Jeremy-Rudy Algorithm)\n",
    "- Step 1: Setting arguments for the ranking system, especially (score_criteria, remove_rate, top_n). Note, score_criteria should be set very carefully otherwise the model could be meaningless.\n",
    "- Step 2: Converting all entries of each numeric column into categorical levels (a, b, c, d, e) according to the (80%, 60%, 40%, 20%) quantiles of the current data.\n",
    "- Step 3: Mark all entries of each numeric column according to the score_criteria given, and then sum the column marks of each merchant (store mark in a new column 'score').\n",
    "- Step 4: Sort all merchants by their mark (descending order) and drop {len(merchant_info) * remove_rate} merchants form tail. \n",
    "- Step 5: Remove the column 'score' and use the merchants left to implement this algorithm again. (Stop until the number  merchants is going to go below 100 after the next run)\n",
    "\n",
    "2. Model Explanation:\n",
    "- Basic Consideration:\n",
    "\t- We give each merchant a rating level (a, b, c, d, e) for each feature, while the rating process is achieved by finding the 80th, 60th, 40th and 20th percentiles of all the data for each feature.\n",
    "\t- Each level could have different marks assigned in different feature, while in all features, except for revenue level, the level e is the worst level.\n",
    "\t- The algorithm is expected to run multiple times, each time a specified percentage of tail (or unwanted) merchants are removed. Then the remaining merchants with score to be reset are prepared for the next run, until we finally obtain the top n merchants.\n",
    "\t- In general, at each run, there will always groups with lower total marks since the Mark Algorithm is based on the current merchants, and we update the remaining merchants at each run.\n",
    "\t- Therefore, for example, at each run, those merchant with all features (except revenue level) to be level 'e' are very likely to be removed as their total marks could be very low and be considered within the drop list\n",
    "\t- In conclusion, for each time we run this algorithm, we don't try to figure out the 'best merchants' as the result could be very unreliable, instead, we aim to find merchants that are cosidered to be the weakest and remove them to ensure accuracy.\n",
    "\n",
    "3. score_criteria:\n",
    "\t- This model will recommend the top n merchants according to our business goal by setting the 'score_criteria' properly.\n",
    "\t- By default, the model will weight all features equally, which means, for instance, the 'transcation_count', 'take_rate' and 'total_revenue' are equally important as criteria for choosing best merchants for BNPL company.\n",
    "\t- However, a 'fair model' is not always a good choice. Actually, a BNPL company may focus more on a merchant's total revenue and take rate, rather than its transaction count as BNPL company can earn more when both the former two terms are high.\n",
    "\t- Therefore, the BNPL company may want to weight more on company with higher 'total_revenue' and 'take_rate', which can be achieved by manually increse the 'score_criteria' for the two terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most important, we perform two ranking systems based on with and without fraud transatcions\n",
    "- Based on this, we found that fraud will not hugely affect a merchant ranking unless the number of fraud transactions outnumber the normal.\n",
    "- Before performing the ranking system, we guess the feature 'revenue level' might be the most important. After seeing the real result, we believe it is the most important feature.\n",
    "- Different segments, have different transaction numbers.\n",
    "- Merchants sell tents usually has advantages in customer puchasing frequency and overall merchant revenue\n",
    "- Investing merchants sell jewlery could maximise BNPL earnings per transaction but also with the highest possibility to be cheated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "In this project, we have meet several limitations. We found ways to solve some of them but we believe there are better ways.\n",
    "1. The lack of external dataset, in the Data preprocess & analysis part, we mentioned that we have linked the external and the given dataset. This way is based on the 2016 correspondence between Postcode and SA2 code. However, there are some postcode can not find its matched SA2 code, we believe if using the latest version of correspondence, we are able to increase the matching precision. Hopefully there is no need to use median filling the null values.\n",
    "2. In the downloading external data part, we didn't find a way to safely extract the data we needed, but we are using url retrieving instead of API calling, this might result cypersecurity risk.\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
